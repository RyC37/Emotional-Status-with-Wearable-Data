{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Author: Sicong Zhao\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "1. Distribution & Correlation of labels\n",
    "2. By subject analysis\n",
    "3. Emotion by `p_score` & `n_score`\n",
    "4. PCA Analysis of Labels\n",
    "5. Chord, emotion transformation\n",
    "6. Would difference in `actual` and `ideal` measures relate to emotion regulation capability?\n",
    "7. KMeans clustering (ha_p, p, la_p) (ha_n, n, la_n), see if we can detect something\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(10, 6)})\n",
    "import copy\n",
    "import datetime\n",
    "import pdb\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode()\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Load the Data & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = pd.read_csv('exp_steps_hr_sleep_survey.csv')\n",
    "# Rmove outliers\n",
    "exp = exp[exp['la_p']!=9]\n",
    "# Generate \n",
    "exp['p_score'] = (exp['ha_p'] + exp['p'] + exp['la_p'])/3\n",
    "exp['n_score'] = (exp['ha_n'] + exp['n'] + exp['la_n'])/3\n",
    "# Label columns\n",
    "label_cols = ['la_p', 'ha_p', 'p', 'p_score', 'ha', 'la_n', 'ha_n', 'n', 'n_score', 'la']\n",
    "# Drop NA\n",
    "exp = exp.dropna(subset=label_cols)\n",
    "exp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Distribution & Correlation of Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix looks reasonable.\n",
    "\n",
    "Distribution wise, `ha_n` and `n` metrics are very right skewed.\n",
    "* About 80% subjects reported `ha_n==1`\n",
    "* About 76% subjects reported `n==1`\n",
    "* About 68% subjects reported `ha_n==1 & n==1`\n",
    "\n",
    "Since our analysis is trying to detect if the subject has negative emotion, could we create our label by using the following criteria:\n",
    "\n",
    "* Normalize by subject.\n",
    "* if `max(la_n, n, ha_n) >= max(la_p, p, ha_p)`, then `label == 1`, which represents the subject is unhappy. (This approach gives 26.6% unhappy label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(df):\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(df, dtype=np.bool))\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    ax = sns.heatmap(df, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "              square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    ax.set_ylim(0,10)\n",
    "corr = exp[label_cols].astype(int).corr()\n",
    "correlation_heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_df = (corr > 0.29) & (corr < 1)\n",
    "corr_df = corr.unstack().reset_index()\n",
    "corr_df.columns = ['ele1','ele2','corr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_df.sort_values(by='corr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Each Labels\n",
    "exp_label = exp[label_cols].stack().reset_index()\n",
    "# Cleaning\n",
    "del exp_label['level_0']\n",
    "exp_label.columns = ['Metrics', 'Score']\n",
    "label_dist = sns.FacetGrid(exp_label, col=\"Metrics\", col_wrap=5)\n",
    "label_dist = (label_dist.map(sns.distplot, \"Score\", hist=True, rug=False, kde = False, bins=range(1, 7, 1), color='#EA6C00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the label based on 2nd rule\n",
    "exp['max_n'] = exp[['la_n','n','ha_n']].max(axis=1)\n",
    "exp['max_p'] = exp[['la_p','p','ha_p']].max(axis=1)\n",
    "exp['valence_bymax'] = exp['max_n'] >= exp['max_p']\n",
    "sum(exp['valence_bymax'] == 1) / exp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp[exp['valence_bymax'] == 1][['la_p','p','ha_p']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp[exp['valence_bymax'] == 0][['la_p','p','ha_p']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.groupby(['ha_n']).count()['subject']/exp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.groupby(['n']).count()['subject']/exp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp[(exp['n']==1)&(exp['ha_n']==1)].shape[0]/exp.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Relationships bewteen emotional states measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the following pattern looks reasonable. One thing worth mentioning is that `n & ha_n`, `ha & ha_n` highly concentrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(col_a, col_b, ax):\n",
    "    ax.title.set_text(col_b + ' & ' + col_a)\n",
    "    exp_ = exp.loc[:,[col_a, col_b]]\n",
    "    exp_.loc[:,'size'] = 1\n",
    "    exp_plot = exp_.groupby([col_a, col_b]).count().reset_index()\n",
    "    sns.scatterplot(x=col_a, y=col_b, data=exp_plot, hue=\"size\", size=\"size\", sizes=(20, 200), ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(9, 5, figsize=(20,40))\n",
    "f.tight_layout()\n",
    "axs = axs.flatten()\n",
    "count = 0\n",
    "for i in range(10):\n",
    "    for j in range(i+1, 10):\n",
    "        plot_scatter(label_cols[i], label_cols[j], ax = axs[count])\n",
    "        count+=1\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.By Subject Analysis\n",
    "\n",
    "The subject level statistics indicates there is huge variance across subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_by_subject = exp[['subject','p_score','n_score']].groupby('subject').agg(['min', 'max','mean','std','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(2, 2, figsize=(16,8))\n",
    "f.suptitle('Distributions of subject-level statistics')\n",
    "axs = axs.flatten()\n",
    "sns.distplot(exp_by_subject['p_score']['mean'].values, bins=20, kde=False, ax=axs[0]);\n",
    "sns.distplot(exp_by_subject['p_score']['std'].values, bins=20, kde=False, ax=axs[1]);\n",
    "sns.distplot(exp_by_subject['n_score']['mean'].values, bins=20, kde=False, ax=axs[2]);\n",
    "sns.distplot(exp_by_subject['n_score']['std'].values, bins=20, kde=False, ax=axs[3]);\n",
    "axs[0].title.set_text('mean of p_score')\n",
    "axs[1].title.set_text('std of p_score')\n",
    "axs[2].title.set_text('mean of n_score')\n",
    "axs[3].title.set_text('std of n_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Valence by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp['Ages'] = None\n",
    "for i in range(20,80,10):\n",
    "    exp.loc[(exp['Age'] >= i)&(exp['Age'] < i+10), 'Ages'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouper of Age\n",
    "count_by_age = pd.DataFrame(exp.groupby('Ages')['subject'].count())\n",
    "count_by_age.columns = ['Count']\n",
    "count_by_age = count_by_age.reset_index()\n",
    "count_by_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "ax = sns.barplot(x=\"Ages\", y=\"Count\", data=count_by_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Positive Score by Age\n",
    "pos_dist = sns.FacetGrid(exp[['Ages','p_score']], col=\"Ages\", col_wrap=3)\n",
    "pos_dist = (pos_dist.map(sns.distplot, \"p_score\", hist=True, rug=False, kde = False,  color='#EA6C00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Positive Score by Age\n",
    "pos_dist = sns.FacetGrid(exp[['Ages','ha_p']], col=\"Ages\", col_wrap=3)\n",
    "pos_dist = (pos_dist.map(sns.distplot, \"ha_p\", hist=True, rug=False, kde = False,  color='#EA6C00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Negative Score by Age\n",
    "neg_dist = sns.FacetGrid(exp[['Ages','n_score']], col=\"Ages\", col_wrap=3)\n",
    "neg_dist = (neg_dist.map(sns.distplot, \"n_score\", hist=True, rug=False, kde = False, color='#4671FB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.PCA Analysis of Labels\n",
    "\n",
    "3 principle components explains 72.8% of all the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_label = exp[label_cols].dropna()\n",
    "exp_label = exp_label[exp_label['la_p']!=9]\n",
    "exp_label_norm = StandardScaler().fit_transform(exp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "principalComponents = pca.fit_transform(exp_label_norm)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2', 'pc3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Emotional States Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze by population**\n",
    "\n",
    "1. Transition probability at all time\n",
    "2. Transition probability in day time\n",
    "3. Transition probability at night"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1) Transition probability of all time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = exp['subject'].unique()\n",
    "exp_label = exp[['subject','start_survey'] + label_cols].dropna()\n",
    "\n",
    "def gen_transition_matrix(df, metric):\n",
    "    tm = [[0]*5 for _ in range(5)]\n",
    "\n",
    "    # Calculation transition numbers\n",
    "    for sub in subjects:\n",
    "        ordered_survey = df.loc[df['subject'] == sub].sort_values(by=['start_survey'])\n",
    "        ordered_survey[metric] = ordered_survey[metric].astype(int)\n",
    "        if ordered_survey.shape[0] > 1:\n",
    "            trans = ordered_survey[metric].values - 1\n",
    "            for (i,j) in zip(trans[:-1],trans[1:]):\n",
    "                tm[i][j] += 1\n",
    "    tm_alltime = copy.deepcopy(tm)\n",
    "    # Calculate transition probabilities\n",
    "    for row in tm:\n",
    "        s = sum(row)\n",
    "        if s > 0:\n",
    "            row[:] = [f/s for f in row]\n",
    "    return tm, tm_alltime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(2, 5, figsize=(26,8))\n",
    "axs = axs.flatten()\n",
    "\n",
    "count = 0\n",
    "for metric in label_cols:\n",
    "    tm, _ = gen_transition_matrix(exp, metric)\n",
    "    sns.heatmap(tm, annot=True, xticklabels=range(1,6), yticklabels=range(1,6), ax=axs[count])\n",
    "    axs[count].title.set_text(metric)\n",
    "    axs[count].set_ylim(0,5)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2) Transition probability of day time vs before/after sleep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the transitions from during daytime and cross days\n",
    "exp['start_survey'] = pd.to_datetime(exp['start_survey'])\n",
    "exp['date'] = exp['start_survey'].dt.date\n",
    "exp_sorted = exp.sort_values(by=['subject','start_survey']).reset_index(drop=True)\n",
    "# Shift\n",
    "exp_sorted['prev_date'] = exp_sorted['date'].shift(1)\n",
    "exp_sorted['prev_subject'] = exp_sorted['subject'].shift(1)\n",
    "# Split data into day-time transition and cross-sleep transition\n",
    "day_trans = exp_sorted[(exp_sorted['prev_date'] == exp_sorted['date']) & \\\n",
    "                            (exp_sorted['prev_subject'] == exp_sorted['subject'])]\n",
    "night_trans = exp_sorted[(exp_sorted['prev_date'] + datetime.timedelta(days=1) == exp_sorted['date']) & \\\n",
    "                            (exp_sorted['prev_subject'] == exp_sorted['subject'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_var = np.subtract(tm_day, tm_night)**2\n",
    "rmse = np.sqrt(np.mean(tm_var))\n",
    "case_count = np.add(day_count, night_count)\n",
    "case_dist = case_count/case_count.sum()\n",
    "rmse_weighted = np.sqrt(np.sum(case_dist * tm_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.89083025e-02*0.11115214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.subtract(tm_day, tm_night)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(10, 4, figsize=(20,42))\n",
    "axs = axs.flatten()\n",
    "\n",
    "count = 0\n",
    "for metric in label_cols:\n",
    "    day = count * 4\n",
    "    night = day + 1\n",
    "    day_case = day + 2\n",
    "    night_case = day + 3\n",
    "    tm_day, day_count = gen_transition_matrix(day_trans, metric)\n",
    "    tm_night, night_count = gen_transition_matrix(night_trans, metric)\n",
    "    # Stats that indicate difference\n",
    "    tm_var = np.subtract(tm_day, tm_night)**2\n",
    "    rmse = np.sqrt(np.mean(tm_var))\n",
    "    case_count = np.add(day_count, night_count)\n",
    "    case_dist = case_count/case_count.sum()\n",
    "    rmse_weighted = np.sqrt(np.sum(case_dist * tm_var))\n",
    "    \n",
    "    sns.heatmap(tm_day, annot=True, xticklabels=range(1,6), yticklabels=range(1,6), ax=axs[day])\n",
    "    sns.heatmap(tm_night, annot=True, xticklabels=range(1,6), yticklabels=range(1,6), ax=axs[night])\n",
    "    sns.heatmap(day_count, annot=True, xticklabels=range(1,6), yticklabels=range(1,6), ax=axs[day_case])\n",
    "    sns.heatmap(night_count, annot=True, xticklabels=range(1,6), yticklabels=range(1,6), ax=axs[night_case])\n",
    "    axs[day].title.set_text('Day: ' + metric + ' Diff: ' + str(rmse))\n",
    "    axs[night].title.set_text('Night: ' + metric)\n",
    "    axs[day_case].title.set_text('Day Count: ' + metric + 'W Diff: ' + str(rmse_weighted))\n",
    "    axs[night_case].title.set_text('Night Count: ' + metric)\n",
    "    for ele in [day, night, day_case, night_case]:\n",
    "        axs[ele].set_ylim(0,5)\n",
    "\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Emotional StatesTransformation by Age group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Major Finding:** For la_n, people at 50s deals with it much better than people at 20s.\n",
    "* With previous state to be `la_n=5`, the 50s have 78% chance to settle down to `la_n=1 or 2` in the next state, while the probability for the 20s is 61%.\n",
    "* With previous state to be `la_n=4`, the 50s have 72% chance to settle down to `la_n=1 or 2` in the next state, while the probability for the 20s is 52%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in label_cols:\n",
    "    f, axs = plt.subplots(1, 6, figsize=(32,4))\n",
    "    f.suptitle(metric + ' by Age')\n",
    "    axs = axs.flatten('F')\n",
    "\n",
    "    count = 0\n",
    "    for age in range(20,80,10):\n",
    "        exp_age = exp[exp['Ages'] == age]\n",
    "        size = exp_age.shape[0]\n",
    "        tm = gen_transition_matrix(exp_age, metric)\n",
    "        sns.heatmap(tm, annot=True, xticklabels=range(1,6), yticklabels=range(1,6), ax=axs[count])\n",
    "        axs[count].title.set_text(metric + ' at ' + str(age) + '(size: ' + str(size) + ')')\n",
    "        axs[count].set_ylim(0,5)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc(exp_label, metric):\n",
    "    # Initialize the matrix\n",
    "    tm = [[0]*5 for _ in range(5)]\n",
    "\n",
    "    # Calculation transition numbers\n",
    "    for sub in subjects:\n",
    "        ordered_survey = exp_label.loc[exp_label['subject'] == sub].sort_values(by=['start_survey'])\n",
    "        ordered_survey[metric] = ordered_survey[metric].astype(int)\n",
    "        if ordered_survey.shape[0] > 1:\n",
    "            trans = ordered_survey[metric].values - 1\n",
    "            for (i,j) in zip(trans[:-1],trans[1:]):\n",
    "                tm[i][j] += 1\n",
    "    tm_alltime = copy.deepcopy(tm)\n",
    "\n",
    "    # Calculate transition probabilities\n",
    "    for row in tm:\n",
    "        s = sum(row)\n",
    "        if s > 0:\n",
    "            row[:] = [f/s for f in row]\n",
    "    tp_alltime = copy.deepcopy(tm)\n",
    "    plt.figure(figsize=(6,4))\n",
    "#     sns.set(rc={'figure.figsize':(6, 4)})\n",
    "    plt.title('Transition Matrix of: '+ metric+ ' for All Subjects')\n",
    "    ax = sns.heatmap(tp_alltime, annot=True, xticklabels=range(1,6), yticklabels=range(1,6))\n",
    "    ax.set_ylim(0,5)\n",
    "    try:\n",
    "        # Calculate stationary distributions\n",
    "        evals, evecs = np.linalg.eig(np.array(tm).T)\n",
    "#         pdb.set_trace()\n",
    "        evec1 = evecs[:,np.isclose(evals, 1)][:,0]\n",
    "        stationary = evec1 / evec1.sum()\n",
    "        #eigs finds complex eigenvalues and eigenvectors, so you'll want the real part.\n",
    "        stationary = stationary.real\n",
    "        plt.figure(figsize=(8,3))\n",
    "        stationary_plot(stationary, metric)\n",
    "    except:\n",
    "        print('Stationary Distribution of: '+ metric+' could not calculate')\n",
    "    \n",
    "def stationary_plot(stationary, metric):\n",
    "    sns.set(rc={'figure.figsize':(6, 2)})\n",
    "    plt.xticks(np.arange(5), ('1','2','3','4','5'))\n",
    "    fig = plt.bar(np.arange(5), stationary, color=sns.color_palette(\"Blues\",5))\n",
    "    # plt.legend(fig, ['First','Second','Third'], loc = \"upper left\", title = \"cat\")\n",
    "    plt.title('Stationary Distribution of: '+ metric+ ' for All Subjects')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc(exp_label, 'ha_p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the heatmap above, it is weired to see the transition between 4, 5 are rare.\n",
    "For subjects in state 1-3 their emotional states tend to stay the same, and gradually become a little happier/sad. \n",
    "\n",
    "However, things are different for happy subjects.\n",
    "For subjects in state 4, they become sadder with probability of 0.66\n",
    "For subjects in state 5, they become sadder with probability of 0.7\n",
    "\n",
    "One possible explaination might be high arousal positive turned to positive or low arousal positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in label_cols+['valence_p','valence_n']:\n",
    "    mc(exp_label, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Day time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the transitions from during daytime and cross days\n",
    "exp['start_survey'] = pd.to_datetime(exp['start_survey'])\n",
    "exp['date'] = exp['start_survey'].dt.date\n",
    "exp_sorted = exp.sort_values(by=['subject','start_survey']).reset_index(drop=True)\n",
    "# Shift\n",
    "exp_sorted['prev_date'] = exp_sorted['date'].shift(1)\n",
    "exp_sorted['prev_subject'] = exp_sorted['subject'].shift(1)\n",
    "# Split data into day-time transition and cross-sleep transition\n",
    "day_trans = exp_label_sorted[(exp_label_sorted['prev_date'] == exp_label_sorted['date']) & \\\n",
    "                            (exp_label_sorted['prev_subject'] == exp_label_sorted['subject'])]\n",
    "night_trans = exp_label_sorted[(exp_label_sorted['prev_date'] + datetime.timedelta(days=1) == exp_label_sorted['date']) & \\\n",
    "                            (exp_label_sorted['prev_subject'] == exp_label_sorted['subject'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_trans = exp_label_sorted[(exp_label_sorted['prev_date'] == exp_label_sorted['date']) & \\\n",
    "                            (exp_label_sorted['prev_subject'] == exp_label_sorted['subject'])]\n",
    "night_trans = exp_label_sorted[(exp_label_sorted['prev_date'] + datetime.timedelta(days=1) == exp_label_sorted['date']) & \\\n",
    "                            (exp_label_sorted['prev_subject'] == exp_label_sorted['subject'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "day_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "night_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in label_cols+['valence_p','valence_n']:\n",
    "    mc(day_trans, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in label_cols+['valence_p','valence_n']:\n",
    "    mc(night_trans, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "night_trans.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) By Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_by_subject(exp_label, metric):\n",
    "    # Initialize the matrix\n",
    "    tm = [[0]*5 for _ in range(5)]\n",
    "\n",
    "    # Calculation transition numbers\n",
    "    ordered_survey = exp_label.sort_values(by=['start_survey'])\n",
    "    ordered_survey[metric] = ordered_survey[metric].astype(int)\n",
    "    if ordered_survey.shape[0] > 1:\n",
    "        trans = ordered_survey[metric].values - 1\n",
    "        for (i,j) in zip(trans[:-1],trans[1:]):\n",
    "            tm[i][j] += 1\n",
    "        tm_alltime = copy.deepcopy(tm)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # Calculate transition probabilities\n",
    "    for row in tm:\n",
    "        s = sum(row)\n",
    "        if s > 0:\n",
    "            row[:] = [f/s for f in row]\n",
    "    tp_alltime = copy.deepcopy(tm)\n",
    "    try:\n",
    "        # Calculate stationary distributions\n",
    "        evals, evecs = np.linalg.eig(np.array(tm).T)\n",
    "#         pdb.set_trace()\n",
    "        evec1 = evecs[:,np.isclose(evals, 1)][:,0]\n",
    "        stationary = evec1 / evec1.sum()\n",
    "        stationary = stationary.real\n",
    "        return stationary\n",
    "    except:\n",
    "        print('Stationary Distribution of: '+ metric+' could not calculate')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate stationary distribution of positive valence and negative valence for each subject\n",
    "exp_stationary_p = exp_label.groupby('subject').apply(lambda x: stationary_by_subject(x,'valence_p'))\n",
    "exp_stationary_n = exp_label.groupby('subject').apply(lambda x: stationary_by_subject(x,'valence_n'))\n",
    "# Formatting the result\n",
    "exp_sta_p_df = pd.DataFrame(exp_stationary_p).reset_index()\n",
    "exp_sta_n_df = pd.DataFrame(exp_stationary_n).reset_index()\n",
    "exp_sta_p_df.columns = ['subject', 'p_state']\n",
    "exp_sta_n_df.columns = ['subject', 'n_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge stationary distribution into design matrix\n",
    "exp_sta_p = pd.merge(exp, exp_sta_p_df, how='left', left_on='subject', right_on='subject') \n",
    "exp_sta_pn = pd.merge(exp_sta_p, exp_sta_n_df, how='left', left_on='subject', right_on='subject') \n",
    "# Split each stationary distribution into five columns\n",
    "exp_sta_pn[['p1_sta','p2_sta','p3_sta','p4_sta','p5_sta']] = pd.DataFrame(exp_sta_pn['p_state'].values.tolist(), index= exp_sta_pn.index)\n",
    "exp_sta_pn[['n1_sta','n2_sta','n3_sta','n4_sta','n5_sta']] = pd.DataFrame(exp_sta_pn['n_state'].values.tolist(), index= exp_sta_pn.index)\n",
    "# Delete old columns\n",
    "del exp_sta_pn['n_state']\n",
    "del exp_sta_pn['p_state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right approach for doing this is after aplitting data, add this feature. In this way, we can prevent information leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part7 KMeans Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide K\n",
    "\n",
    "# function returns WSS score for k values from 1 to kmax\n",
    "def calculate_WSS(points, kmax):\n",
    "  sse = []\n",
    "  for k in range(1, kmax+1):\n",
    "    kmeans = KMeans(n_clusters = k).fit(points)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    pred_clusters = kmeans.predict(points)\n",
    "    curr_sse = 0\n",
    "    \n",
    "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
    "    for i in range(len(points)):\n",
    "      curr_center = centroids[pred_clusters[i]]\n",
    "      curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
    "      \n",
    "    sse.append(curr_sse)\n",
    "  return sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = exp[['ha_p','p','la_p']].dropna().values\n",
    "X_neg = exp[['ha_p','p','la_p']].dropna().values\n",
    "\n",
    "wss_pos_res = calculate_WSS(X_pos, 100)\n",
    "wss_neg_res = calculate_WSS(X_neg, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wss_pos = pd.DataFrame({'k-value': range(1,31), 'WSS': wss_pos_res[:30]})\n",
    "plt.title('Within-Cluster-Sum of Squared over K for positive emotion')\n",
    "sns.set(rc={'figure.figsize':(10, 6)})\n",
    "sns.lineplot(x='k-value', y='WSS', data=wss_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wss_neg = pd.DataFrame({'k-value': range(1,31), 'WSS': wss_neg_res[:30]})\n",
    "plt.title('Within-Cluster-Sum of Squared over K for negative emotion')\n",
    "sns.set(rc={'figure.figsize':(10, 6)})\n",
    "sns.lineplot(x='k-value', y='WSS', data=wss_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = exp[['ha_p','p','la_p']].dropna()\n",
    "X_neg = exp[['ha_n','n','la_n']].dropna()\n",
    "\n",
    "\n",
    "def cluster_plot(X, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X.values)\n",
    "    X['clusters'] = kmeans.labels_\n",
    "    x_col = X.columns.values\n",
    "    fig = px.scatter_3d(X, x=x_col[0], y=x_col[1], z=x_col[2], color='clusters')\n",
    "    fig.show()\n",
    "\n",
    "cluster_plot(X_pos, 4)\n",
    "cluster_plot(X_neg, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
